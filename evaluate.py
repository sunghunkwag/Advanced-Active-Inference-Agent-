import torch
import numpy as np
import os

from environment import PixelEnv
from agent import AgentController
from baseline_agent import BaselineAgentController
from world_model import VAE
from transition_model import TransitionModel
from context_engine import ContextInferenceEngine
from memory import EpisodicMemory
from baseline_models import BaselineVAE, BaselineTransitionModel

# --- Configuration ---
EVAL_TASKS = [10, 11, 12, 13, 14] # Use task IDs unseen during training
EPISODE_LENGTH = 200 # How many steps to evaluate on for each task
IMG_SIZE = 32 # Must match the trained models

# --- Paths for Pre-trained Models ---
# In a real scenario, these would be generated by a long training run.
# We will check if they exist, otherwise we use randomly initialized models.
VAE_PATH = "saved_models/vae.pth"
TRANSITION_PATH = "saved_models/transition.pth"
CONTEXT_PATH = "saved_models/context.pth"

def run_evaluation_episode(agent, env, episode_length, agent_type="meta"):
    """Runs one episode of evaluation for a given agent."""
    if agent_type == "meta":
        agent.memory.reset() # Reset memory for the new task

    obs = env.reset()
    total_reward = 0

    for _ in range(episode_length):
        action = agent.select_action(obs)
        obs, reward, done, _ = env.step(action)

        if agent_type == "meta":
            # The meta-agent uses its memory to infer context in real-time
            agent.record_experience(obs, action, obs, reward)

        total_reward += reward
        if done:
            break

    return total_reward

def main():
    print("--- Starting Agent Evaluation ---")

    # --- 1. Initialize Environment for Evaluation ---
    env = PixelEnv(size=IMG_SIZE, num_tasks=max(EVAL_TASKS) + 1)

    # --- 2. Load Meta-Learning Agent ---
    print("\nLoading Meta-Learning Agent...")
    # These would be the parameters found by tuning
    latent_dim, context_dim, hidden_dim = 32, 16, 256

    vae = VAE(latent_dim, context_dim, 1, IMG_SIZE)
    transition_model = TransitionModel(latent_dim, env.get_num_actions(), context_dim, hidden_dim)
    context_engine = ContextInferenceEngine((1, IMG_SIZE, IMG_SIZE), env.get_num_actions(), context_dim, hidden_dim)
    memory = EpisodicMemory(200, (1, IMG_SIZE, IMG_SIZE), 1)

    # Load pre-trained weights if available
    if os.path.exists(VAE_PATH) and os.path.exists(TRANSITION_PATH) and os.path.exists(CONTEXT_PATH):
        vae.load_state_dict(torch.load(VAE_PATH))
        transition_model.load_state_dict(torch.load(TRANSITION_PATH))
        context_engine.load_state_dict(torch.load(CONTEXT_PATH))
        print("Loaded pre-trained models.")
    else:
        print("Pre-trained models not found. Using randomly initialized weights.")
        print("(Note: Meaningful evaluation requires pre-trained models from running main.py)")

    meta_agent = AgentController(
        vae, transition_model, context_engine, memory, env.get_num_actions(), latent_dim, context_dim
    )

    # --- 3. Initialize Baseline Agent ---
    print("\nInitializing Baseline Agent...")
    baseline_vae = BaselineVAE(latent_dim, 1, IMG_SIZE)
    baseline_transition = BaselineTransitionModel(latent_dim, env.get_num_actions(), hidden_dim)
    baseline_agent = BaselineAgentController(
        baseline_vae, baseline_transition, env.get_num_actions(), latent_dim
    )

    # --- 4. Run Evaluation Loop ---
    meta_agent_rewards = []
    baseline_agent_rewards = []

    for task_id in EVAL_TASKS:
        print(f"\n--- Evaluating on Unseen Task #{task_id} ---")
        env.reset_task(task_id)

        # Evaluate Meta-Agent
        meta_reward = run_evaluation_episode(meta_agent, env, EPISODE_LENGTH, agent_type="meta")
        meta_agent_rewards.append(meta_reward)
        print(f"  Meta-Agent Total Reward: {meta_reward:.2f}")

        # Evaluate Baseline Agent
        # The baseline agent cannot adapt, so its performance is expected to be poor.
        baseline_reward = run_evaluation_episode(baseline_agent, env, EPISODE_LENGTH, agent_type="baseline")
        baseline_agent_rewards.append(baseline_reward)
        print(f"  Baseline Agent Total Reward: {baseline_reward:.2f}")

    # --- 5. Report Final Results ---
    avg_meta_reward = np.mean(meta_agent_rewards)
    avg_baseline_reward = np.mean(baseline_agent_rewards)

    print("\n\n--- Evaluation Summary ---")
    print(f"Tasks Evaluated: {EVAL_TASKS}")
    print(f"Average Reward (Meta-Agent): {avg_meta_reward:.2f}")
    print(f"Average Reward (Baseline Agent): {avg_baseline_reward:.2f}")

    if avg_meta_reward > avg_baseline_reward:
        print("\nConclusion: The Meta-Learning Agent demonstrates superior adaptation capabilities.")
    else:
        print("\nConclusion: The Meta-Learning Agent did not outperform the baseline.")
        print("(This is expected if using randomly initialized models.)")

if __name__ == '__main__':
    main()
